# =============================================================================
# ChatNShop - Environment Configuration Example
# =============================================================================
# Copy this file to .env and fill in your actual values
# Never commit .env to version control!

# =============================================================================
# APPLICATION SETTINGS
# =============================================================================
APP_NAME=Intent Classification API
APP_VERSION=1.0.0
HOST=0.0.0.0
PORT=8000
WORKERS=1
LOG_LEVEL=info

# CORS Configuration
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8000

# Environment
ENVIRONMENT=development

# =============================================================================
# OPENAI API CONFIGURATION
# =============================================================================
# Required: Get your API key from https://platform.openai.com/
OPENAI_API_KEY=your_openai_api_key_here

# Model Configuration
OPENAI_MODEL=gpt-4-turbo
GPT4_MODEL=gpt-4
GPT4_TURBO_MODEL=gpt-4-turbo
GPT35_MODEL=gpt-3.5-turbo

# OpenAI Request Settings
OPENAI_TEMPERATURE=0.3
OPENAI_MAX_TOKENS=400
OPENAI_TIMEOUT_SECS=30

# Development Mode (set to true to avoid real API calls during testing)
DRY_RUN=false

# =============================================================================
# QDRANT VECTOR DATABASE
# =============================================================================
QDRANT_HOST=localhost
QDRANT_PORT=6333
QDRANT_URL=http://localhost:6333
QDRANT_API_KEY=mykey@123

# =============================================================================
# REDIS CONFIGURATION
# =============================================================================
# Option 1: Use REDIS_URL (recommended)
REDIS_URL=redis://localhost:6379/0

# Option 2: Or specify individual components
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=

# =============================================================================
# LLM CACHING CONFIGURATION
# =============================================================================
# Enable/disable LLM response caching
ENABLE_LLM_CACHE=true

# Cache similarity threshold (0.0-1.0, higher = stricter matching)
LLM_CACHE_SIMILARITY_THRESHOLD=0.95

# Cache TTL in seconds (86400 = 24 hours)
LLM_CACHE_TTL=86400

# Maximum cache size (number of entries)
LLM_CACHE_MAX_SIZE=10000

# Minimum query length for caching (prevents caching single words)
LLM_CACHE_MIN_QUERY_LENGTH=3

# =============================================================================
# CONTEXT & SESSION MANAGEMENT
# =============================================================================
# Enable conversation history context enhancement
ENABLE_SESSION_CONTEXT=true

# Maximum tokens for context enhancement
CONTEXT_TOKEN_LIMIT=2000

# Number of conversation messages to include in context
CONTEXT_HISTORY_LIMIT=5

# =============================================================================
# QUEUE CONFIGURATION (Optional)
# =============================================================================
# Enable/disable async queue processing (set to false for direct synchronous calls)
ENABLE_LLM_QUEUE=false

# =============================================================================
# ALERTING & MONITORING
# =============================================================================
# Slack webhook for cost alerts (optional)
COST_ALERT_SLACK_WEBHOOK=

# Cost alert thresholds
COST_ALERT_DAILY_THRESHOLD_USD=2.0
ACCURACY_DROP_THRESHOLD=0.05
LATENCY_SPIKE_THRESHOLD_MS=3000
ERROR_RATE_SPIKE_THRESHOLD=0.05

# Escalation webhook for critical alerts (optional)
ESCALATION_WEBHOOK_URL=
